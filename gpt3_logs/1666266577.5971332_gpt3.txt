The following is an unstructured video transcript of a machine-learning lecture. Make a list of the main topics discussed in this transcript.

TRANSCRIPT:

criteria are met. First, there is sufficient and good data available. That's the input. Second, there is an easy to measure ground rules that we want to predict. That's how our output. Third, it is easy to use feedback to steadily improve the model. And finally, there is a clearly defined quality measure, which we've been talking about a lot now, that we want to optimize. So let's look at an example. Let's take Facebook for example. Facebook has over two billion users. So that's a pretty good amount of training data. So the ground rule is also pretty easy to measure, because it's clicks on post pictures, videos. And it's also pretty easy for them to get feedback. As every click of a user indicates that the prediction was correct. And every time a user doesn't click, shows the opposite. And finally, there's a clear quality measure that they want to optimize. We all know it's revenue by showing ads. But even under such perfect conditions, damage can be made by using the machine on models. Like radicalization is one of them, also discrimination. And there are basically four ways how this can happen. We have to go back to the case of Bridger Bowden, who apparently got a higher risk, because it's just because of her skin color. From an algorithm that should actually eliminate such discrimination. Well, algorithms try to separate people into different categories. So they discriminate so to speak. So this is a problem when ever sensible information has been used to do that. Like imagine ethnic origin, gender, religion, disabilities, age, sexual orientation, and so on. And a good example for that is the hiring process we just spoke about. Like Anderson Google or Facebook, they tried for years to predict which candidates were a good fit for them. As input data for their training. Of course, they use data of people they have hired in the last 10 years. So the problem, as you look at the screen is only every fifth employee in these companies were females. And also the algorithm views the gender as input variable. It was able to derive character is six that correlate with the gender like. Memberships in women's gyms female colleges or even the motivation line, which is totally different depending on gender or cultural background. So when your training data already contains discrimination. The algorithm will only amplify it even without the knowledge of sensible information. And what is the result? The result is even fewer female hires. So the second way how discrimination can happen is because of missing data. Even without Alexa, Cortana or Siri, we all know that controlling things with our voice will only get bigger and bigger. But did you know that these systems have mainly have been trained with the voice of white males without any accent at all. And that's a problem because like for example in Australia, when you apply for working visa, there's a speech recognition so that will evaluate if you're speaking well enough English. And to receive a visa after all. And somebody with a heavy accent won't pass this test as the system just haven't been trained for that. Right. So then the third possibility is quite interesting because discrimination can be provoked also by omitting sensible data for training. To show you what I mean, let's go back to our data set with your real lapsing criminals. Remember, there was no line that was able to separate guilty and innocent people perfectly. But maybe that changes if you add some sensible information like gender to it. How does it look now? When we separate the data by gender female to the left, male to the right. Suddenly it's perfectly possible to find a solution with no error at all. If we put the data back together, this ain't possible because the red line, the one that only makes for errors, clearly discriminates male citizens. You see that? Two of them are classified as criminals, even though they're actually innocent. And on the other side, two guilty female citizens, they are found innocent. Nice. Good. So the first possibility, how discrimination can get in your data is called continuous learning. And there's a lot of you have heard of Ty. This was actually a couple of years back and it was a chatbot from Microsoft. And you should talk to people on Twitter and learn from their questions and answers. And at the beginning, this worked pretty well, but then some users targeted him with racist or sexist comments. And soon enough, he was replying the same things. And this is kind of a good example of what we call garbage in garbage out. So you could use like automatic filters so that that stuff doesn't get posted. But the question again is, which kind of error am I willing to accept? Is it worse to censor a post that was actually harmless or is it worse to censor racist post? To not censor racist post. So social media platforms like May Facebook, they have their own cleaners. That scant from millions of posts per day and delete content that shouldn't be shown to their users. So you can imagine if there was a better way of weighing freedom of speech against human dignity, I'm I'm pretty sure those companies will do it. So that's I think that's about it for my quick little introduction. Just just to sum it up, what I really tried to bring a grass in these couple of slides were mainly two things. Okay, with a couple of years down the road, I already said you pretty sure end up in some kind of leadership goal and you get approached by say it's people that promise you that the machine learning model is not on 99% better or detects 99% of x y set or is 90% more efficient than x y set. Then you know you should pay attention attention to what kind of quality measure goes used for training. Did they define a clear goal what the model should predict and is this the same goal your company has in mind. Like in the example with the higher in process. And second, the second thing I wanted to bring across is this if decisions of a machine learning model will affect


LIST OF MAIN TOPICS:
-


==========

-The use of machine learning in various fields
-The potential for machine learning models to discriminate against certain groups of people
-How to avoid discrimination in machine learning models